PREVIOUS DIARIZATION IMPLEMENTATION SUMMARY
===========================================

Before the switch to Pyannote Audio 3.1, the system used a "clustering-based" approach combining Whisper segments with SpeechBrain embeddings.

DEPENDENCIES
------------
- speechbrain
- scikit-learn
- torchaudio
- numpy

KEY COMPONENTS
--------------
1.  **Model**: `speechbrain/spkrec-ecapa-voxceleb`
    - A lightweight ECAPA-TDNN model for extracting speaker embeddings.
    
2.  **Algorithm**:
    a.  **Transcription**: Run Whisper to get text segments + timestamps.
    b.  **Embedding Extraction**: 
        - Iterate through each Whisper segment.
        - Crop the audio for that segment.
        - If segment > 0.5s, generate a 192-dimensional vector (embedding) using SpeechBrain.
    c.  **Clustering**:
        - Normalize embeddings (L2 norm).
        - If `num_speakers` is NOT provided:
          - Logic: Run a loop for `k` from 2 to 15.
          - Calculate `silhouette_score` for each `k`.
          - Pick the `k` with the highest score.
        - Use `AgglomerativeClustering` (Hierarchical clustering) with `metric="euclidean"` and `linkage="ward"`.
    d.  **Labeling**:
        - Assign the resulting cluster label (0, 1, 2...) to the text segment.

CODE SNIPPETS (Restoration Reference)
-------------------------------------

Imports:
    from speechbrain.inference.speaker import EncoderClassifier
    from sklearn.cluster import AgglomerativeClustering
    from sklearn.metrics import silhouette_score

Initialization:
    self.speaker_encoder = EncoderClassifier.from_hparams(
        source="speechbrain/spkrec-ecapa-voxceleb",
        savedir=local_sb_path,
        run_opts={"device": "cuda"} if self.device == "cuda" else None
    )

Diarize Method Logic:
    # ... (After extracting embeddings into a list) ...
    
    # Auto-detect k
    best_score = -1
    best_k = 2
    for k in range(2, min(len(embeddings), 15)):
        temp_cluster = AgglomerativeClustering(n_clusters=k)
        labels = temp_cluster.fit_predict(normalized_embeddings)
        score = silhouette_score(normalized_embeddings, labels)
        if score > best_score:
            best_k = k
            
    # Final fit
    clustering = AgglomerativeClustering(n_clusters=best_k)
    labels = clustering.fit_predict(normalized_embeddings)

WHY WE CHANGED IT
-----------------
- **Ghost Speakers**: Short audio segments produced noisy embeddings, leading the clustering algorithm to think there were far more speakers (e.g., 150) than reality.
- **Overlapping Speech**: This approach could only assign 1 speaker per text segment. It could not detect two people talking at once (Pyannote can).
